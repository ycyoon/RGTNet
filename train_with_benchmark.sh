#!/bin/bash

# RGTNet DDP í•™ìŠµ ë° ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

echo "ğŸš€ Starting RGTNet DDP training with benchmark evaluation..."

# GPU ë©”ëª¨ë¦¬ ì²´í¬
echo "ğŸ“Š Checking GPU memory..."
nvidia-smi

# ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p models results

# 7ê°œì˜ GPUë¥¼ ì‚¬ìš©í•˜ì—¬ DDPë¡œ main.py ì‹¤í–‰ (GPU 0ì€ vLLM ì„œë²„ìš©ìœ¼ë¡œ ì œì™¸)
# batch_size 1, gradient_accumulation_steps 8 -> ì‹¤ì§ˆì ì¸ ë°°ì¹˜ í¬ê¸° 1*7*8=56
# --use_amp : ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ Automatic Mixed Precision ì‚¬ìš©
echo "ğŸ”¥ Starting training with 7 GPUs..."
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 torchrun --nproc_per_node=7 --master_port=29600 main.py \
    --pretrained_model_name meta-llama/Meta-Llama-3-8B \
    --epochs 3 \
    --batch_size 1 \
    --gradient_accumulation_steps 8 \
    --use_amp \
    --lr 5e-5 \
    --save_path "models/llama3_8b_rgtnet.pth" \
    --results_file "results/llama3_8b_rgtnet_results.json" \
    --enable_benchmark \
    --benchmark_freq 1 \
    --dropout 0.1 \
    --bias_delta 1.0 \
    --weight_decay 0.01 \
    --warmup_ratio 0.1 \
    --download_datasets \
    --gradient_checkpointing \
    --max_seq_len 2048
#    --max_iters 100 \
#    --benchmark_dir "StructTransformBench/benchmark" \

echo "âœ… DDP Training completed!"


# # ë…¼ë¬¸ ë°©ì‹ StructTransform ë²¤ì¹˜ë§ˆí¬ í‰ê°€ (HarmBench judge + Refusal judge)
# echo "\nğŸš¦ ë…¼ë¬¸ ë°©ì‹ StructTransform ë²¤ì¹˜ë§ˆí¬ í‰ê°€ (HarmBench judge + Refusal judge) ì‹¤í–‰..."
# python structtransform_benchmark.py \
#     --model_path "models/${MODEL_NAME}.pth" \
#     --output_path "results/${MODEL_NAME}_structtransform_benchmark.json" \
#     --benchmark_dir "StructTransformBench/benchmark" \
#     --tokenizer_name "bert-base-uncased" \
#     --d_model $D_MODEL \
#     --nhead $NHEAD \
#     --num_layers $NUM_LAYERS \
#     --max_seq_len $MAX_SEQ_LEN \
#     --dropout 0.1 \
#     --bias_delta 1.0 \
#     --batch_size 8 \
#     --device cuda

# echo "ğŸ‰ All done! Check the results directory for analysis."
