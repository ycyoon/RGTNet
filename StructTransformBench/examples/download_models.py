#!/usr/bin/env python
"""
ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
HuggingFace ëª¨ë¸ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ìºì‹±
"""
import os
import sys
from huggingface_hub import snapshot_download
from transformers import AutoTokenizer, AutoModelForCausalLM

# ì˜¤í”„ë¼ì¸ ëª¨ë“œ ë¹„í™œì„±í™”
os.environ.pop("HF_HUB_OFFLINE", None)
os.environ.pop("TRANSFORMERS_OFFLINE", None)

# í† í° ì„¤ì •
hf_token = os.environ.get("HF_TOKEN")
if not hf_token:
    print("âš ï¸ HF_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ HF_TOKENì„ ì„¤ì •í•´ì£¼ì„¸ìš”: export HF_TOKEN=your_token_here")
    sys.exit(1)

def download_model(model_id):
    """ì§€ì •ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    print(f"ğŸ”„ {model_id} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    try:
        # ìŠ¤ëƒ…ìƒ· ë‹¤ìš´ë¡œë“œ
        snapshot_download(
            repo_id=model_id,
            token=hf_token,
            local_dir=f"./models/{model_id.split('/')[-1]}",
            local_dir_use_symlinks=False
        )
        
        # í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)
        
        # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (íŒŒë¼ë¯¸í„° ë¡œë“œ ì—†ì´)
        model_info = AutoModelForCausalLM.from_pretrained(
            model_id, 
            token=hf_token,
            device_map=None,  # ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì§€ ì•ŠìŒ
            torch_dtype="auto"
        )
        
        print(f"âœ… {model_id} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ {model_id} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False

# ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ëª©ë¡
models = [
    "meta-llama/Llama-3.2-3B-Instruct",
    "meta-llama/Meta-Llama-3.2-1B-Instruct",
    "deepseek-ai/deepseek-llm-7b-chat",
    "deepseek-ai/deepseek-coder-6.7b-instruct",
]

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
success_count = 0
for model in models:
    if download_model(model):
        success_count += 1

print(f"\nğŸ“Š ê²°ê³¼: {success_count}/{len(models)} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
