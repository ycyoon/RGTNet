#!/usr/bin/env python3
"""
CPU ë³‘ëª© ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import time
import psutil
import threading
from collections import defaultdict
import multiprocessing as mp

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import pynvml
    pynvml.nvmlInit()
    NVIDIA_AVAILABLE = True
except:
    NVIDIA_AVAILABLE = False

class PerformanceMonitor:
    def __init__(self):
        self.monitoring = False
        self.data = defaultdict(list)
        self.monitor_thread = None
        
    def start_monitoring(self):
        """Start performance monitoring"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        print("ğŸ” Performance monitoring started...")
        
    def stop_monitoring(self):
        """Stop performance monitoring"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1)
        print("â¹ï¸  Performance monitoring stopped")
        
    def _monitor_loop(self):
        """Monitoring loop"""
        while self.monitoring:
            timestamp = time.time()
            
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=None, percpu=True)
            avg_cpu = sum(cpu_percent) / len(cpu_percent)
            self.data['cpu_avg'].append((timestamp, avg_cpu))
            self.data['cpu_cores'].append((timestamp, cpu_percent))
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory = psutil.virtual_memory()
            self.data['memory_percent'].append((timestamp, memory.percent))
            self.data['memory_available_gb'].append((timestamp, memory.available / 1024**3))
            
            # í”„ë¡œì„¸ìŠ¤ë³„ CPU ì‚¬ìš©ë¥ 
            try:
                current_process = psutil.Process()
                self.data['process_cpu'].append((timestamp, current_process.cpu_percent()))
                self.data['process_memory_mb'].append((timestamp, current_process.memory_info().rss / 1024**2))
                self.data['process_threads'].append((timestamp, current_process.num_threads()))
            except:
                pass
            
            # GPU ì‚¬ìš©ë¥  (ê°€ëŠ¥í•œ ê²½ìš°)
            if TORCH_AVAILABLE and torch.cuda.is_available():
                try:
                    gpu_memory = torch.cuda.memory_allocated() / 1024**3
                    gpu_reserved = torch.cuda.memory_reserved() / 1024**3
                    self.data['gpu_memory_allocated_gb'].append((timestamp, gpu_memory))
                    self.data['gpu_memory_reserved_gb'].append((timestamp, gpu_reserved))
                except:
                    pass
            
            if NVIDIA_AVAILABLE:
                try:
                    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                    gpu_util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    self.data['gpu_utilization'].append((timestamp, gpu_util.gpu))
                    self.data['gpu_memory_utilization'].append((timestamp, gpu_util.memory))
                except:
                    pass
            
            time.sleep(0.5)  # Monitor every 0.5 seconds
    
    def print_summary(self):
        """Print performance summary"""
        if not self.data:
            print("âŒ No monitoring data available")
            return
            
        print("\n" + "="*60)
        print("ğŸ” CPU ë³‘ëª© ë¶„ì„ ê²°ê³¼")
        print("="*60)
        
        # CPU ë¶„ì„
        if 'cpu_avg' in self.data:
            cpu_values = [v for t, v in self.data['cpu_avg']]
            avg_cpu = sum(cpu_values) / len(cpu_values)
            max_cpu = max(cpu_values)
            print(f"ğŸ’» CPU ì‚¬ìš©ë¥ :")
            print(f"   í‰ê· : {avg_cpu:.1f}%")
            print(f"   ìµœëŒ€: {max_cpu:.1f}%")
            
            if avg_cpu > 80:
                print("   ğŸš¨ CPU ë³‘ëª© ê°ì§€! (í‰ê·  80% ì´ìƒ)")
            elif avg_cpu > 60:
                print("   âš ï¸  CPU ì‚¬ìš©ë¥  ë†’ìŒ (í‰ê·  60% ì´ìƒ)")
            else:
                print("   âœ… CPU ì‚¬ìš©ë¥  ì •ìƒ")
        
        # ë©”ëª¨ë¦¬ ë¶„ì„
        if 'memory_percent' in self.data:
            memory_values = [v for t, v in self.data['memory_percent']]
            avg_memory = sum(memory_values) / len(memory_values)
            max_memory = max(memory_values)
            print(f"\nğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ :")
            print(f"   í‰ê· : {avg_memory:.1f}%")
            print(f"   ìµœëŒ€: {max_memory:.1f}%")
            
            if avg_memory > 90:
                print("   ğŸš¨ ë©”ëª¨ë¦¬ ë¶€ì¡±! (í‰ê·  90% ì´ìƒ)")
            elif avg_memory > 75:
                print("   âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ (í‰ê·  75% ì´ìƒ)")
            else:
                print("   âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì •ìƒ")
        
        # í”„ë¡œì„¸ìŠ¤ ë¶„ì„
        if 'process_cpu' in self.data:
            process_cpu_values = [v for t, v in self.data['process_cpu']]
            avg_process_cpu = sum(process_cpu_values) / len(process_cpu_values)
            max_process_cpu = max(process_cpu_values)
            print(f"\nğŸƒ í”„ë¡œì„¸ìŠ¤ ì„±ëŠ¥:")
            print(f"   í‰ê·  CPU: {avg_process_cpu:.1f}%")
            print(f"   ìµœëŒ€ CPU: {max_process_cpu:.1f}%")
            
            if 'process_threads' in self.data:
                thread_values = [v for t, v in self.data['process_threads']]
                avg_threads = sum(thread_values) / len(thread_values)
                max_threads = max(thread_values)
                print(f"   í‰ê·  ìŠ¤ë ˆë“œ: {avg_threads:.0f}")
                print(f"   ìµœëŒ€ ìŠ¤ë ˆë“œ: {max_threads:.0f}")
        
        # GPU ë¶„ì„
        gpu_available = False
        if 'gpu_utilization' in self.data:
            gpu_util_values = [v for t, v in self.data['gpu_utilization']]
            if gpu_util_values:
                avg_gpu = sum(gpu_util_values) / len(gpu_util_values)
                max_gpu = max(gpu_util_values)
                print(f"\nğŸ® GPU ì‚¬ìš©ë¥ :")
                print(f"   í‰ê· : {avg_gpu:.1f}%")
                print(f"   ìµœëŒ€: {max_gpu:.1f}%")
                gpu_available = True
                
                if avg_gpu < 20:
                    print("   ğŸš¨ GPU ì €ì‚¬ìš©! (í‰ê·  20% ë¯¸ë§Œ) - CPU ë³‘ëª© ê°€ëŠ¥ì„± ë†’ìŒ")
                elif avg_gpu < 50:
                    print("   âš ï¸  GPU ì‚¬ìš©ë¥  ë‚®ìŒ (í‰ê·  50% ë¯¸ë§Œ)")
                else:
                    print("   âœ… GPU ì˜ í™œìš©ë¨")
        
        if 'gpu_memory_allocated_gb' in self.data:
            gpu_mem_values = [v for t, v in self.data['gpu_memory_allocated_gb']]
            if gpu_mem_values:
                avg_gpu_mem = sum(gpu_mem_values) / len(gpu_mem_values)
                max_gpu_mem = max(gpu_mem_values)
                print(f"\nğŸ® GPU ë©”ëª¨ë¦¬:")
                print(f"   í‰ê· : {avg_gpu_mem:.1f} GB")
                print(f"   ìµœëŒ€: {max_gpu_mem:.1f} GB")
        
        # ë³‘ëª© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ë³‘ëª© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­:")
        
        cpu_high = 'cpu_avg' in self.data and sum([v for t, v in self.data['cpu_avg']]) / len(self.data['cpu_avg']) > 70
        gpu_low = 'gpu_utilization' in self.data and sum([v for t, v in self.data['gpu_utilization']]) / len(self.data['gpu_utilization']) < 30
        
        if cpu_high and gpu_low:
            print("   ğŸ¯ CPU ë³‘ëª© ê°ì§€!")
            print("   ğŸ“Œ í•´ê²° ë°©ì•ˆ:")
            print("      - ë°°ì¹˜ í¬ê¸° ì¦ê°€")
            print("      - ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜ ì¦ê°€")
            print("      - ë°ì´í„° ë¡œë”© ìµœì í™”")
            print("      - í† í¬ë‚˜ì´ì € ë³‘ë ¬í™”")
        elif cpu_high:
            print("   ğŸ¯ CPU ì‚¬ìš©ë¥  ë†’ìŒ")
            print("   ğŸ“Œ CPU ìµœì í™” í•„ìš”")
        elif gpu_low and gpu_available:
            print("   ğŸ¯ GPU ì €ì‚¬ìš© ê°ì§€")
            print("   ğŸ“Œ GPU í™œìš©ë„ ê°œì„  í•„ìš”")
        else:
            print("   âœ… ì „ë°˜ì ì¸ ì„±ëŠ¥ ì–‘í˜¸")
        
        print("="*60)

def simulate_cpu_intensive_task():
    """CPU ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜"""
    print("ğŸ”„ CPU ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜ ì¤‘...")
    
    # í† í¬ë‚˜ì´ì € ë³‘ëª© ì‹œë®¬ë ˆì´ì…˜
    import time
    import threading
    
    def cpu_task():
        # ë¬¸ìì—´ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ (í† í¬ë‚˜ì´ì €ì™€ ìœ ì‚¬)
        for i in range(100000):
            text = f"This is a sample text for processing {i}" * 10
            words = text.split()
            processed = " ".join(words[:50])
    
    # ì—¬ëŸ¬ ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ì²˜ë¦¬
    threads = []
    num_threads = os.cpu_count() or 4
    
    start_time = time.time()
    for i in range(num_threads):
        t = threading.Thread(target=cpu_task)
        threads.append(t)
        t.start()
    
    for t in threads:
        t.join()
    
    end_time = time.time()
    print(f"âœ… CPU ì‘ì—… ì™„ë£Œ (ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")

def simulate_gpu_task():
    """GPU ì‘ì—… ì‹œë®¬ë ˆì´ì…˜"""
    if not TORCH_AVAILABLE or not torch.cuda.is_available():
        print("âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥")
        return
    
    print("ğŸ”„ GPU ì‘ì—… ì‹œë®¬ë ˆì´ì…˜ ì¤‘...")
    
    # ê°„ë‹¨í•œ í–‰ë ¬ ì—°ì‚°
    device = torch.device('cuda')
    
    start_time = time.time()
    for i in range(10):
        a = torch.randn(1000, 1000, device=device)
        b = torch.randn(1000, 1000, device=device)
        c = torch.matmul(a, b)
        torch.cuda.synchronize()
    
    end_time = time.time()
    print(f"âœ… GPU ì‘ì—… ì™„ë£Œ (ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")

def main():
    print("ğŸš€ CPU ë³‘ëª© ë¶„ì„ í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    print(f"ğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"   CPU ì½”ì–´: {os.cpu_count()}")
    print(f"   ë©”ëª¨ë¦¬: {psutil.virtual_memory().total / 1024**3:.1f} GB")
    
    if TORCH_AVAILABLE:
        print(f"   PyTorch: ì‚¬ìš© ê°€ëŠ¥")
        if torch.cuda.is_available():
            print(f"   CUDA: ì‚¬ìš© ê°€ëŠ¥ (GPU: {torch.cuda.get_device_name()})")
        else:
            print(f"   CUDA: ì‚¬ìš© ë¶ˆê°€ëŠ¥")
    else:
        print(f"   PyTorch: ì‚¬ìš© ë¶ˆê°€ëŠ¥")
    
    # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = PerformanceMonitor()
    monitor.start_monitoring()
    
    try:
        # CPU ì§‘ì•½ì  ì‘ì—… ì‹¤í–‰
        simulate_cpu_intensive_task()
        
        # ì ì‹œ ëŒ€ê¸°
        time.sleep(2)
        
        # GPU ì‘ì—… ì‹¤í–‰
        simulate_gpu_task()
        
        # ì ì‹œ ë” ëŒ€ê¸°
        time.sleep(3)
        
    finally:
        # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ ë° ê²°ê³¼ ì¶œë ¥
        monitor.stop_monitoring()
        monitor.print_summary()

if __name__ == "__main__":
    main()
