#rgtnet ëª¨ë¸ í…ŒìŠ¤íŠ¸ - DeepSpeed ì²´í¬í¬ì¸íŠ¸ìš©

import torch
from model_hybrid import create_hybrid_model
from transformers import AutoTokenizer
import argparse
import os
import glob
from safetensors.torch import load_file
from datetime import datetime

def find_latest_model():
    """ìµœì‹  ëª¨ë¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
    base_dir = "/home/ycyoon/work/RGTNet/models"
    
    # 1. ë¨¼ì € symbolic link í™•ì¸
    latest_link = os.path.join(base_dir, "rgtnet_final_model_latest")
    if os.path.islink(latest_link):
        link_target = os.readlink(latest_link)
        full_path = os.path.join(base_dir, link_target)
        if os.path.exists(full_path):
            return full_path
    
    # 2. ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ì—ì„œ ìµœì‹  ì°¾ê¸°
    pattern = os.path.join(base_dir, "rgtnet_final_model_*")
    model_dirs = glob.glob(pattern)
    
    if not model_dirs:
        return None
    
    # ë‚ ì§œì™€ ì‹œê°„ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    model_dirs.sort(key=os.path.getctime, reverse=True)
    return model_dirs[0]

def list_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ëª©ë¡ ì¶œë ¥"""
    base_dir = "/home/ycyoon/work/RGTNet/models"
    
    print("ğŸ“‹ Available models:")
    print("="*50)
    
    # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”
    models_by_date = {}
    
    pattern = os.path.join(base_dir, "rgtnet_final_model_*")
    model_dirs = glob.glob(pattern)
    
    for model_dir in sorted(model_dirs, key=os.path.getctime, reverse=True):
        dirname = os.path.basename(model_dir)
        date_str = dirname.replace("rgtnet_final_model_", "")
        
        if len(date_str) >= 8:  # YYYYMMDD_HHMMSS í˜•ì‹
            date_part = date_str[:8]
            time_part = date_str[9:15] if len(date_str) > 8 else ""
            
            if date_part not in models_by_date:
                models_by_date[date_part] = []
            
            models_by_date[date_part].append((time_part, model_dir))
    
    # ë‚ ì§œë³„ë¡œ ì¶œë ¥
    for date in sorted(models_by_date.keys(), reverse=True):
        print(f"\n {date[:4]}-{date[4:6]}-{date[6:8]}:")
        for time_part, model_dir in models_by_date[date]:
            if time_part:
                print(f"  ğŸ• {time_part[:2]}:{time_part[2:4]}:{time_part[4:6]} - {model_dir}")
            else:
                print(f"   {model_dir}")

def load_hybrid_model(model_dir):
    """Hybrid Llama-RGTNet ëª¨ë¸ ë¡œë“œ"""
    print(f" Loading hybrid model from: {model_dir}")
    
    # Check for PEFT adapter files
    adapter_files = glob.glob(os.path.join(model_dir, "adapter_*.bin"))
    adapter_config = os.path.join(model_dir, "adapter_config.json")
    
    if adapter_files and os.path.exists(adapter_config):
        print(f" Found PEFT adapter files: {[os.path.basename(f) for f in adapter_files]}")
        return model_dir  # Return directory path for PEFT loading
    
    # Check for regular model files (fallback)
    model_files = []
    for pattern in ["model.safetensors", "pytorch_model.bin", "model-*.safetensors", "pytorch_model-*.bin"]:
        model_files.extend(glob.glob(os.path.join(model_dir, pattern)))
    
    if model_files:
        print(f" Found model files: {[os.path.basename(f) for f in model_files]}")
        return model_dir
    
    print(f"âŒ No model or adapter files found in {model_dir}")
    return None

def create_hybrid_model_args():
    """Hybrid ëª¨ë¸ìš© args ìƒì„±"""
    args = argparse.Namespace(
        pretrained_model_name="meta-llama/Llama-3.2-3B-Instruct",
        enable_role_adapters=True,
        use_quantization=False,
        use_lora=True,  # Enable LoRA for testing
        lora_r=8,
        lora_alpha=16,
        lora_dropout=0.05,
        single_gpu_test=True  # Flag for single GPU testing
    )
    
    return args

def create_single_gpu_hybrid_model(args):
    """í…ŒìŠ¤íŠ¸ìš© ë‹¨ì¼ GPU í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ìƒì„±"""
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
    import torch
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.unk_token if tokenizer.unk_token else tokenizer.eos_token
    
    # Load base model on single GPU
    base_model = AutoModelForCausalLM.from_pretrained(
        args.pretrained_model_name,
        torch_dtype=torch.float16,
        device_map={"": 0},  # Force everything to GPU 0
        trust_remote_code=True,
    )
    
    # Apply LoRA
    if args.use_lora:
        base_model = prepare_model_for_kbit_training(base_model)
        lora_config = LoraConfig(
            r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            bias="none",
            task_type="CAUSAL_LM",
        )
        base_model = get_peft_model(base_model, lora_config)
    
    return base_model, tokenizer

def test_model_generation(model, tokenizer, device, prompts=None):
    """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
    if prompts is None:
        prompts = [
            "Hello, how are you?",
            "What is artificial intelligence?", 
            "Explain quantum computing in simple terms.",
        ]
    
    print("\n" + "="*60)
    print("HYBRID MODEL GENERATION TEST")
    print("="*60)
    
    model.eval()
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\n--- Test {i}: {prompt} ---")
        
        try:
            # Chat template ì ìš©
            messages = [{"role": "user", "content": prompt}]
            formatted_prompt = tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # í† í°í™” with attention mask
            inputs = tokenizer(formatted_prompt, return_tensors="pt", padding=True)
            input_ids = inputs["input_ids"].to(device)
            attention_mask = inputs["attention_mask"].to(device)
            
            print(f"Input tokens shape: {input_ids.shape}")
            print(f"Input prompt (first 100 chars): {formatted_prompt[:100]}...")
            
            # ìƒì„± (ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸)
            with torch.no_grad():
                output = model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=30,
                    do_sample=True, 
                    temperature=0.8,
                    top_p=0.9,
                    top_k=50,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”© - ìƒˆë¡œ ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶œë ¥
            input_length = input_ids.shape[1]
            generated_ids = output[0][input_length:]  # ìƒˆë¡œ ìƒì„±ëœ í† í°ë§Œ
            generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            print(f"Generated response: {generated_text}")
            print(f"Full conversation: {tokenizer.decode(output[0], skip_special_tokens=True)[:200]}...")
            
        except Exception as e:
            print(f"âŒ Error in generation: {e}")
            import traceback
            traceback.print_exc()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ RGTNet DeepSpeed Model Test")
    print("="*60)
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    parser = argparse.ArgumentParser(description='Test RGTNet model')
    parser.add_argument('--model_dir', type=str, help='Specific model directory to test')
    parser.add_argument('--list_models', action='store_true', help='List all available models')
    args = parser.parse_args()
    
    # ëª¨ë¸ ëª©ë¡ ì¶œë ¥ ì˜µì…˜
    if args.list_models:
        list_available_models()
        return
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    if args.model_dir:
        model_dir = args.model_dir
        if not os.path.exists(model_dir):
            print(f"âŒ Specified model directory not found: {model_dir}")
            return
    else:
        # ìµœì‹  ëª¨ë¸ ìë™ ì°¾ê¸°
        model_dir = find_latest_model()
        if model_dir is None:
            print("âŒ No model found!")
            print("Available options:")
            print("1. Use --list_models to see all available models")
            print("2. Use --model_dir to specify a specific model directory")
            return
    
    print(f" Model directory: {model_dir}")
    
    # Hybrid ëª¨ë¸ ì²´í¬
    model_path = load_hybrid_model(model_dir)
    if model_path is None:
        print("âŒ Failed to find model files")
        return
    
    # args ìƒì„±
    model_args = create_hybrid_model_args()
    
    print(f"\nğŸ“‹ Using hybrid model config:")
    print(f"  Base model: {model_args.pretrained_model_name}")
    print(f"  Role adapters: {model_args.enable_role_adapters}")
    print(f"  LoRA enabled: {model_args.use_lora}")
    print(f"  LoRA rank: {model_args.lora_r}")
    
    # ëª¨ë¸ ìƒì„± ë° ë¡œë“œ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nğŸ’» Using device: {device}")
    
    try:
        # Single GPU í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ ìƒì„±
        print("\nğŸ”§ Creating single GPU test model...")
        model, tokenizer = create_single_gpu_hybrid_model(model_args)
        
        # PEFT ì–´ëŒ‘í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ
        adapter_files = glob.glob(os.path.join(model_dir, "adapter_*.bin"))
        if adapter_files:
            print(f" Loading PEFT adapters from {model_dir}...")
            # Note: In a real scenario, you'd use model.load_adapter() or similar
            # For now, just indicate that adapters were found
            print("âœ… PEFT adapter files found (loading would require PEFT integration)")
        
        print("âœ… Single GPU test model created successfully!")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
        print(f"\nğŸ“Š Model info:")
        print(f"  Total parameters: {total_params:,}")
        print(f"  Trainable parameters: {trainable_params:,}")
        print(f"  Model structure: {type(model).__name__}")
        
        print("âœ… Tokenizer loaded successfully!")
        
        # ê¸°ë³¸ forward pass í…ŒìŠ¤íŠ¸
        print("\n" + "="*60)
        print("BASIC FORWARD PASS TEST")
        print("="*60)
        
        model.eval()
        
        # Use GPU 0 for single GPU test
        model_device = torch.device("cuda:0")
        print(f"Model is on device: {model_device}")
        
        test_input = torch.randint(0, 1000, (1, 10)).to(model_device)
        test_role_mask = torch.randint(0, 2, (1, 10)).to(model_device)
        
        with torch.no_grad():
            outputs = model(input_ids=test_input)
            print(f"âœ… Forward pass successful!")
            print(f"Output shape: {outputs.logits.shape}")
            print(f"Expected shape: (1, 10, vocab_size)")
        
        # ìƒì„± í…ŒìŠ¤íŠ¸
        test_model_generation(model, tokenizer, model_device)
        
        print("\nğŸ‰ All tests completed successfully!")
        
    except Exception as e:
        print(f"âŒ Error during model setup: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()